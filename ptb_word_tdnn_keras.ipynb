{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 08/12/18 [07:45 - 13:20]\n",
    "So yeah, I might have done a mistake or two in the last implementation.... Between them, not realising that the input to Conv1D has a lot of channels and that I ~~could~~ should use a lot of filters.  \n",
    "But that's all in the past, now that this little boy is up and learning and growing in a healthy manner.  \n",
    "Now the first Conv1D uses 5 filters and the second one, 3. I could perhaps increase this someday.  \n",
    "Not only that, but now it also saves it weights during training (I had done this in another implementation, but I forgot to commit that one, so now it's in this one).  \n",
    "And now on to some notes for my future self, I was asing myself if it would be possible to get the 16 dimensional vector produced by Embedding and use Conv2D on it. It seems to be possible with the aid of keras.layers.Reshape. The idea would be to simply add a single dimension to the end of Embedding, so it would become 4D, allowing it to be used as input to Conv2D. I might do that in a separate file though, so that I'm able to compare both results.\n",
    "Another possible improvement might be to shuffle the data. I didn't do so because there data order is relevant in this data set, but who knows what might happen?  \n",
    "Also, here goes a complete log of the changes, in chronological order:\n",
    "- Changed the parameter filter in the first Conv1D to 5\n",
    "- Changed the parameter filter in the second Conv2D to 3\n",
    "- Changed metrics to 'acc', as there is no implementation of sparse_categorical_accuracy in tf.keras\n",
    "- Added saving cell, with model.save_weights\n",
    "- Added ModelCheckpoint callback, saving only weights\n",
    "- Added model saving to saving cell\n",
    "- Changed the ModelCheckpoint parameter save_only_weights to False, as to resume training in later ocasions\n",
    "- Also, increased periods in ModelCheckpoint\n",
    "- Changed the checkpoint path to include data and time of checkpoint\n",
    "- Gave names to the layers\n",
    "- Added loading model cell  \n",
    "\n",
    "On a side note, loss just went nuts\n",
    "\n",
    "\n",
    "### Some time in the past:\n",
    "I have yet to conclude training the model, but I was to excited to share it finally working after a long time.  \n",
    "On a side note, however, it does seem to overfit before the end of even the first epoch, but we need to conclude a whole training session before jumping to any conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction using TDNN implemented in tf.keras\n",
    "This notebook is an attempt at implementing a Time Delay Neural Network for word prediction in the ptb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import tensorboard as tf\n",
    "from tensorflow import keras\n",
    "import reader\n",
    "import numpy as np\n",
    "import os\n",
    "from time import strftime, gmtime\n",
    "import pathlib\n",
    "from pathlib import PosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "window_size = 20 # defines the past lookup for determining the following word\n",
    "path = \"data/simple-examples/data\"\n",
    "checkpoint_path = \"training/cp-{epoch:04d}_\" + strftime(\"%H-%M_%d-%m-%Y\", gmtime()) + \".ckpt\"\n",
    "checkpoint_dir = os.path.dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell obtain the data using the reader.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocab_size, word_to_id = reader.ptb_raw_data(path)\n",
    "x_train = train_data[:-1]\n",
    "x_train = [np.asarray(x_train[i:i+window_size]) for i in range(len(x_train)-window_size)]\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(train_data[1:-window_size])\n",
    "#y_train = keras.preprocessing.text.one_hot(y_train, vocab_size)\n",
    "x_valid = valid_data[:-1]\n",
    "x_valid = [np.asarray(x_valid[i:i+window_size]) for i in range(len(x_valid)-window_size)]\n",
    "x_valid = np.asarray(x_valid)\n",
    "y_valid = valid_data[1:-window_size]\n",
    "y_valid = np.asarray(y_valid)\n",
    "x_test = test_data[:-1]\n",
    "x_test = [np.asarray(x_test[i:i+window_size]) for i in range(len(x_test)-window_size)]\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = test_data[1:-window_size]\n",
    "y_test = np.asarray(y_test)\n",
    "id_to_word = {value: key for (key, value) in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following next, we have an auxiliary function which decodes the ids and give us the original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(text):\n",
    "    return ' '.join([id_to_word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos> mr. <unk> is chairman of <unk> n.v. the dutch publishing group <eos> rudolph <unk> N years old and former'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text (x_train[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929568, 20)\n",
      "(929568,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each index of the input has a length of 20 words, as defined in _window-length_\n",
    "<br>\n",
    "With our data already processed, we can finally create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time instantiation\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_length = window_size, name = \"Embedding_1\"))\n",
    "#model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Conv1D(filters = 5, kernel_size = 3, padding = \"same\", activation = keras.activations.tanh, name = \"Conv1D_1\"))\n",
    "model.add(keras.layers.Dropout(0.2, name = \"Droupout_1\"))\n",
    "model.add(keras.layers.Conv1D(filters = 3, kernel_size = 3, padding = \"same\", activation = keras.activations.tanh, name = \"Conv2D_2\"))\n",
    "model.add(keras.layers.Dropout(0.25, name = \"Dropout_2\"))\n",
    "model.add(keras.layers.Flatten(name = \"Flatten_1\"))\n",
    "model.add(keras.layers.Dense(vocab_size, activation = keras.activations.softmax, name = \"Dense_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading - loads the most recent model recorded manually. Feel free to change\n",
    "# Sort the checkpoints by modification time.\n",
    "checkpoints = pathlib.Path(\"./models\").glob(\"*\")\n",
    "checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "latest = str(checkpoints[-1])\n",
    "checkpoints\n",
    "model = keras.models.load_model(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 20, 5)             245       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 5)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 20, 3)             48        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 3)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10000)             610000    \n",
      "=================================================================\n",
      "Total params: 770,293\n",
      "Trainable params: 770,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer = keras.optimizers.Adadelta(),\n",
    "    metrics = ['acc'] #keras.metrics.categorical_accuracy] # remember to later change to sparse_categorical_accuracy (this is the cause for strange eval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose = 1, save_weights_only = False, period = 3) # Also, later change to save_weights_only = false, and perhaps increase period to 3 or 4. This will allow us to later resume training from where we left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 929568 samples, validate on 73739 samples\n",
      "Epoch 1/12\n",
      "929568/929568 [==============================] - 1211s 1ms/step - loss: 4.6491 - acc: 0.3793 - val_loss: 4.1737 - val_acc: 0.4720\n",
      "Epoch 2/12\n",
      "929568/929568 [==============================] - 1199s 1ms/step - loss: 4.6510 - acc: 0.3801 - val_loss: 4.1709 - val_acc: 0.4738\n",
      "Epoch 3/12\n",
      "929568/929568 [==============================] - 1177s 1ms/step - loss: 4.6551 - acc: 0.3789 - val_loss: 4.1730 - val_acc: 0.4738\n",
      "\n",
      "Epoch 00003: saving model to training/cp-000315-20_12-08-2018.ckpt\n",
      "Epoch 4/12\n",
      "929568/929568 [==============================] - 1178s 1ms/step - loss: 4.6563 - acc: 0.3794 - val_loss: 4.1722 - val_acc: 0.4736\n",
      "Epoch 5/12\n",
      "929568/929568 [==============================] - 1184s 1ms/step - loss: 4.6575 - acc: 0.3793 - val_loss: 4.1719 - val_acc: 0.4744\n",
      "Epoch 6/12\n",
      "929568/929568 [==============================] - 1220s 1ms/step - loss: 4.6589 - acc: 0.3793 - val_loss: 4.1735 - val_acc: 0.4735\n",
      "\n",
      "Epoch 00006: saving model to training/cp-000615-20_12-08-2018.ckpt\n",
      "Epoch 7/12\n",
      "929568/929568 [==============================] - 1279s 1ms/step - loss: 4.6613 - acc: 0.3790 - val_loss: 4.1699 - val_acc: 0.4740\n",
      "Epoch 8/12\n",
      "929568/929568 [==============================] - 1419s 2ms/step - loss: 4.6622 - acc: 0.3791 - val_loss: 4.1741 - val_acc: 0.4740\n",
      "Epoch 9/12\n",
      "929568/929568 [==============================] - 1131s 1ms/step - loss: 4.6611 - acc: 0.3795 - val_loss: 4.1691 - val_acc: 0.4750\n",
      "\n",
      "Epoch 00009: saving model to training/cp-000915-20_12-08-2018.ckpt\n",
      "Epoch 10/12\n",
      "929568/929568 [==============================] - 1249s 1ms/step - loss: 4.6625 - acc: 0.3793 - val_loss: 4.1742 - val_acc: 0.4749\n",
      "Epoch 11/12\n",
      "929568/929568 [==============================] - 1335s 1ms/step - loss: 4.6665 - acc: 0.3786 - val_loss: 4.1727 - val_acc: 0.4728\n",
      "Epoch 12/12\n",
      "929568/929568 [==============================] - 1465s 2ms/step - loss: 4.6628 - acc: 0.3792 - val_loss: 4.1712 - val_acc: 0.4739\n",
      "\n",
      "Epoch 00012: saving model to training/cp-001215-20_12-08-2018.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7fc176af7b00>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs = 12,\n",
    "          verbose = 1,\n",
    "          validation_data = (x_valid, y_valid),\n",
    "          shuffle = False,\n",
    "          callbacks = [cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.126658921275957\n",
      "Test accuracy: 0.4793044449057557\n"
     ]
    }
   ],
   "source": [
    "# I expect to be able to run this someday\n",
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model_weights/weights_\" + strftime(\"%H-%M_%d-%m-%Y\", gmtime()))\n",
    "model.save(\"./models/model_\" + strftime(\"%H-%M_%d-%m-%Y\", gmtime())) # This allows us to resume training, since Adadelta has adaptive parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
