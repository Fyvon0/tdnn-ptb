{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 02/09/18 [12:00]\n",
    "Here are some notes after \"finishing training\".  \n",
    "To kick things off, each epoch took aproximately 2h20min to train, in my personal Ubuntu. It isn't the greatest of machines, but I was able to get some very impressive results after 5 epochs:  \n",
    "_Test loss: 1.4660126214812637  \n",
    "Test accuracy: 0.8552342583948307_  \n",
    "  \n",
    "The lengthy duration of each epoch is the reason for why I didn't continue training for more epochs. And also the fact that python got to the point of consuming 1GB of RAM, making my computer downright unusable.  \n",
    "Given that, the model results were saved after those 5 epochs.  \n",
    "One of the reasons for such a long training time is that using 2d convolution makes it so that the model has way more trainable parameters, and I mean it:  \n",
    "_Total params: 9,770,188  \n",
    "Trainable params: 9,770,188  \n",
    "Non-trainable params: 0_  \n",
    "  \n",
    "Also, something that I found during my research is this contrib feature: https://www.tensorflow.org/api_docs/python/tf/contrib/data/sliding_window_batch  \n",
    "I'm not entirely sure of how to use it, but it might prove itself very useful if I ever need to train a network on some fixed data, so the network itself should do the sliding window data preparation.  \n",
    "  \n",
    "That being said, I'm happy with the obtained results. And as a last note, it's noticeable that the model began calibrating less and less after time, and I believe that is due to the optimizer used, the _Adadelta_, which has a adaptive momentum implementation. I speculate that using a SGD, this model could have reached accuracy close to the 90%-95% range within the same 5 epochs.  \n",
    "  \n",
    "### 01/09/18 [14:00 - 14:30]\n",
    "As I mentioned in the last note, I tried implementing a 2d Convolution over the word Embeddings, almost as if they were images.  \n",
    "To do so, I added a Reshape layer between the Embedding_1 and Conv1d_1, in which the final shape was (batch_size, window_size, embedding_dimensionality, 1). After that, I changed the 1d convolutions to 2d, changing the kernel_size from 3 to (3,3). With that, I can now explain the the addition of an apparently useless new dimensionality to the input vector after Reshape, and I can confirm that it is indeed useless. I mean, kinda. What it does is that conv2d requires a 4d input, with one of the dimensionalities being number of channels. So all that Reshape_1 does is that it includes a new \"null\" dimension that is treated as the number of channels, so that we can do spatial convolution on the Embeddings. \n",
    "And the results appear to be clear from the very start. I've only ran half an epoch so far, but accuracy results appear promising.  \n",
    "\n",
    "### 12/08/18 [07:45 - 13:20]\n",
    "So yeah, I might have done a mistake or two in the last implementation.... Between them, not realising that the input to Conv1D has a lot of channels and that I ~~could~~ should use a lot of filters.  \n",
    "But that's all in the past, now that this little boy is up and learning and growing in a healthy manner.  \n",
    "Now the first Conv1D uses 5 filters and the second one, 3. I could perhaps increase this someday.  \n",
    "Not only that, but now it also saves it weights during training (I had done this in another implementation, but I forgot to commit that one, so now it's in this one).  \n",
    "And now on to some notes for my future self, I was asing myself if it would be possible to get the 16 dimensional vector produced by Embedding and use Conv2D on it. It seems to be possible with the aid of keras.layers.Reshape. The idea would be to simply add a single dimension to the end of Embedding, so it would become 4D, allowing it to be used as input to Conv2D. I might do that in a separate file though, so that I'm able to compare both results.\n",
    "Another possible improvement might be to shuffle the data. I didn't do so because there data order is relevant in this data set, but who knows what might happen?  \n",
    "Also, here goes a complete log of the changes, in chronological order:\n",
    "- Changed the parameter filter in the first Conv1D to 5\n",
    "- Changed the parameter filter in the second Conv2D to 3\n",
    "- Changed metrics to 'acc', as there is no implementation of sparse_categorical_accuracy in tf.keras\n",
    "- Added saving cell, with model.save_weights\n",
    "- Added ModelCheckpoint callback, saving only weights\n",
    "- Added model saving to saving cell\n",
    "- Changed the ModelCheckpoint parameter save_only_weights to False, as to resume training in later ocasions\n",
    "- Also, increased periods in ModelCheckpoint\n",
    "- Changed the checkpoint path to include data and time of checkpoint\n",
    "- Gave names to the layers\n",
    "- Added loading model cell  \n",
    "\n",
    "On a side note, loss just went nuts\n",
    "\n",
    "\n",
    "### Some time in the past:\n",
    "I have yet to conclude training the model, but I was to excited to share it finally working after a long time.  \n",
    "On a side note, however, it does seem to overfit before the end of even the first epoch, but we need to conclude a whole training session before jumping to any conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction using TDNN implemented in tf.keras\n",
    "This notebook is an attempt at implementing a Time Delay Neural Network for word prediction in the ptb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import print_function\n",
    "import tensorboard as tb\n",
    "from tensorflow import keras\n",
    "import reader\n",
    "import numpy as np\n",
    "import os\n",
    "from time import strftime, gmtime\n",
    "import pathlib\n",
    "from pathlib import PosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "window_size = 20 # defines the past lookup for determining the following word\n",
    "path = \"data/simple-examples/data\"\n",
    "checkpoint_path = \"training/conv2d/cp-{epoch:04d}_\" + strftime(\"%H-%M_%d-%m-%Y\", gmtime()) + \".ckpt\"\n",
    "checkpoint_dir = os.path.dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell obtain the data using the reader.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocab_size, word_to_id = reader.ptb_raw_data(path)\n",
    "x_train = train_data[:-1]\n",
    "x_train = [np.asarray(x_train[i:i+window_size]) for i in range(len(x_train)-window_size)]\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(train_data[1:-window_size])\n",
    "#y_train = keras.preprocessing.text.one_hot(y_train, vocab_size)\n",
    "x_valid = valid_data[:-1]\n",
    "x_valid = [np.asarray(x_valid[i:i+window_size]) for i in range(len(x_valid)-window_size)]\n",
    "x_valid = np.asarray(x_valid)\n",
    "y_valid = valid_data[1:-window_size]\n",
    "y_valid = np.asarray(y_valid)\n",
    "x_test = test_data[:-1]\n",
    "x_test = [np.asarray(x_test[i:i+window_size]) for i in range(len(x_test)-window_size)]\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = test_data[1:-window_size]\n",
    "y_test = np.asarray(y_test)\n",
    "id_to_word = {value: key for (key, value) in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following next, we have an auxiliary function which decodes the ids and give us the original sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(text):\n",
    "    return ' '.join([id_to_word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos> mr. <unk> is chairman of <unk> n.v. the dutch publishing group <eos> rudolph <unk> N years old and former'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text (x_train[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929568, 20)\n",
      "(929568,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each index of the input has a length of 20 words, as defined in _window-length_\n",
    "<br>\n",
    "With our data already processed, we can finally create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time instantiation\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_length = window_size, name = \"Embedding_1\"))\n",
    "#model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Reshape((20,16,1), name = \"Reshape_1\"))\n",
    "model.add(keras.layers.Conv2D(filters = 5, kernel_size = (3,3), padding = \"same\", activation = keras.activations.tanh, name = \"Conv2D_1\"))\n",
    "model.add(keras.layers.Dropout(0.2, name = \"Droupout_1\"))\n",
    "model.add(keras.layers.Conv2D(filters = 3, kernel_size = 3, padding = \"same\", activation = keras.activations.tanh, name = \"Conv2D_2\"))\n",
    "model.add(keras.layers.Dropout(0.25, name = \"Dropout_2\"))\n",
    "model.add(keras.layers.Flatten(name = \"Flatten_1\"))\n",
    "model.add(keras.layers.Dense(vocab_size, activation = keras.activations.softmax, name = \"Dense_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading - loads the most recent model recorded manually. Feel free to change\n",
    "# Sort the checkpoints by modification time.\n",
    "checkpoints = pathlib.Path(\"./models\").glob(\"*\")\n",
    "checkpoints = sorted(checkpoints, key=lambda cp:cp.stat().st_mtime)\n",
    "checkpoints = [cp.with_suffix('') for cp in checkpoints]\n",
    "latest = str(checkpoints[-1])\n",
    "checkpoints\n",
    "model = keras.models.load_model(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_1 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "Reshape_1 (Reshape)          (None, 20, 16, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 20, 16, 5)         50        \n",
      "_________________________________________________________________\n",
      "Droupout_1 (Dropout)         (None, 20, 16, 5)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 20, 16, 3)         138       \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 20, 16, 3)         0         \n",
      "_________________________________________________________________\n",
      "Flatten_1 (Flatten)          (None, 960)               0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 10000)             9610000   \n",
      "=================================================================\n",
      "Total params: 9,770,188\n",
      "Trainable params: 9,770,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer = keras.optimizers.Adadelta(),\n",
    "    metrics = ['acc'] #keras.metrics.categorical_accuracy] # remember to later change to sparse_categorical_accuracy (this is the cause for strange eval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose = 1, save_weights_only = False, period = 3) # Also, later change to save_weights_only = false, and perhaps increase period to 3 or 4. This will allow us to later resume training from where we left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 929568 samples, validate on 73739 samples\n",
      "Epoch 1/12\n",
      "929568/929568 [==============================] - 7905s 9ms/step - loss: 1.9855 - acc: 0.7735 - val_loss: 1.7232 - val_acc: 0.8218\n",
      "Epoch 2/12\n",
      "929568/929568 [==============================] - 7865s 8ms/step - loss: 1.8789 - acc: 0.7881 - val_loss: 1.6392 - val_acc: 0.8333\n",
      "Epoch 3/12\n",
      "929568/929568 [==============================] - 7881s 8ms/step - loss: 1.8093 - acc: 0.7969 - val_loss: 1.5854 - val_acc: 0.8404\n",
      "\n",
      "Epoch 00003: saving model to training/conv2d/cp-0003_20-54_01-09-2018.ckpt\n",
      "Epoch 4/12\n",
      "929568/929568 [==============================] - 7894s 8ms/step - loss: 1.7597 - acc: 0.8030 - val_loss: 1.5460 - val_acc: 0.8456\n",
      "Epoch 5/12\n",
      "929568/929568 [==============================] - 8130s 9ms/step - loss: 1.7246 - acc: 0.8075 - val_loss: 1.5162 - val_acc: 0.8492\n",
      "Epoch 6/12\n",
      " 22048/929568 [..............................] - ETA: 2:12:19 - loss: 1.8817 - acc: 0.7875"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7468986f8d60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs = 12,\n",
    "          verbose = 1,\n",
    "          validation_data = (x_valid, y_valid),\n",
    "          shuffle = False,\n",
    "          callbacks = [cp_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.4660126214812637\n",
      "Test accuracy: 0.8552342583948307\n"
     ]
    }
   ],
   "source": [
    "# I expect to be able to run this someday\n",
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model_weights/conv2d_weights_\" + strftime(\"%H-%M_%d-%m-%Y\", gmtime()))\n",
    "model.save(\"./models/conv2d_model_\" + strftime(\"%H-%M_%d-%m-%Y\", gmtime())) # This allows us to resume training, since Adadelta has adaptive parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
